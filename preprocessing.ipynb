{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62394e28",
   "metadata": {},
   "source": [
    "# 🚀 **Complete Machine Learning Preprocessing Guide**\n",
    "*An Interactive Tutorial for Data Scientists*\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "            color: white; \n",
    "            padding: 20px; \n",
    "            border-radius: 15px; \n",
    "            margin: 20px 0; \n",
    "            text-align: center;\n",
    "            box-shadow: 0 8px 32px rgba(0,0,0,0.1);\">\n",
    "    <h2 style=\"margin: 0; font-size: 1.8em;\">📊 Master Data Preprocessing Like a Pro</h2>\n",
    "    <p style=\"margin: 5px 0 0 0; opacity: 0.9;\">Click on each section below to explore comprehensive preprocessing techniques</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 **Table of Contents**\n",
    "<div style=\"background: rgba(0,123,255,0.1); \n",
    "            border-left: 4px solid #007bff; \n",
    "            padding: 15px; \n",
    "            margin: 15px 0; \n",
    "            border-radius: 0 10px 10px 0;\">\n",
    "\n",
    "1. **[Data Understanding & Inspection](#1-data-understanding--inspection)** 🔍\n",
    "2. **[Handling Missing Data](#2-handling-missing-data)** 🕳️\n",
    "3. **[Handling Outliers](#3-handling-outliers)** 📈\n",
    "4. **[Data Type Conversion](#4-data-type-conversion)** 🔄\n",
    "5. **[Encoding Categorical Variables](#5-encoding-categorical-variables)** 🏷️\n",
    "6. **[Feature Scaling](#6-feature-scaling)** ⚖️\n",
    "7. **[Feature Engineering](#7-feature-engineering)** 🛠️\n",
    "8. **[Feature Selection](#8-feature-selection)** 🎯\n",
    "9. **[Text Preprocessing](#9-text-preprocessing)** 📝\n",
    "10. **[Data Splitting](#10-data-splitting)** ✂️\n",
    "11. **[Balancing the Dataset](#11-balancing-the-dataset)** ⚖️\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #28a745, #20c997); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🔍 1. Data Understanding & Inspection\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(40, 167, 69, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose** \n",
    "Get familiar with your dataset structure and identify potential issues early.\n",
    "\n",
    "### **🎯 Key Steps**\n",
    "- Load the dataset using appropriate libraries (`pandas`, `numpy`)\n",
    "- Understand the structure: `.shape`, `.info()`, `.describe()`\n",
    "- Visual inspection: `.head()`, `.tail()`, `.sample()`\n",
    "- Identify data types (numerical, categorical, datetime, text, etc.)\n",
    "- Check for duplicate records\n",
    "\n",
    "### **✅ Best Practices**\n",
    "<div style=\"background: rgba(255, 193, 7, 0.1); border-left: 4px solid #ffc107; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "- 👉 **Always start with data profiling**\n",
    "- 👉 **Document your findings**\n",
    "- 👉 **Look for patterns in missing data**\n",
    "- 👉 **Check data consistency across columns**\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Code Example**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Basic inspection\n",
    "print(f\"Dataset shape    : {df.shape}\")\n",
    "print(f\"Dataset info     : {df.info()}\")\n",
    "print(f\"Basic statistics :\\n{df.describe()}\")\n",
    "print(f\"Random sample    :\\n{df.sample(5)}\")\n",
    "print(f\"Missing values   :\\n{df.isnull().sum()}\")\n",
    "print(f\"Duplicated rows  : {df.duplicated().sum()}\")\n",
    "\n",
    "# Visual inspection\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "display(df.sample(5))  # Random sample\n",
    "\n",
    "# Quick visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "df.hist(bins=20, ax=axes.flatten()[:len(df.select_dtypes(include=[np.number]).columns)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #dc3545, #fd7e14); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🕳️ 2. Handling Missing Data\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(220, 53, 69, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Deal with incomplete data that can negatively impact model performance.\n",
    "\n",
    "### **🔍 Detection Methods**\n",
    "```python\n",
    "# Count missing values\n",
    "df.isnull().sum()\n",
    "df.isnull().sum().sum()  # Total missing values\n",
    "\n",
    "# Visualize missing patterns\n",
    "import missingno as msno\n",
    "msno.matrix(df)\n",
    "msno.heatmap(df)\n",
    "```\n",
    "\n",
    "### **🛠️ Strategies**\n",
    "\n",
    "#### **1️⃣ Deletion Approach**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 10px; margin: 10px 0;\">\n",
    "\n",
    "- **Drop rows**: When missing data is random and dataset is large ⚠️\n",
    "- **Drop columns**: When >70% values are missing ⚠️\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Imputation Approach**\n",
    "\n",
    "**📊 Numerical Data:**\n",
    "- **Mean** (for normal distribution)\n",
    "- **Median** (for skewed data, robust to outliers)\n",
    "- **Mode** (for categorical-like numerical data)\n",
    "\n",
    "**🏷️ Categorical Data:**\n",
    "- **Mode** (most frequent value)\n",
    "- **Create \"Unknown\" category**\n",
    "\n",
    "**🚀 Advanced Methods:**\n",
    "- **KNN Imputation**\n",
    "- **Iterative Imputation**\n",
    "- **Forward/Backward fill** (for time series)\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "import missingno as msno\n",
    "\n",
    "# Visualize missing values\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Simple imputation for numerical data\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Simple imputation for categorical data\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "# KNN imputation (more sophisticated)\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_numeric_knn = knn_imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Iterative imputation\n",
    "iterative_imputer = IterativeImputer(random_state=42)\n",
    "df_iterative = iterative_imputer.fit_transform(df[numerical_cols])\n",
    "```\n",
    "\n",
    "<div style=\"background: rgba(13, 202, 240, 0.1); border-left: 4px solid #0dcaf0; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**💡 Pro Tip:** Always analyze the **missing data pattern** before choosing a strategy. Random missing data can be imputed, but systematic missing patterns might indicate data collection issues.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #6f42c1, #e83e8c); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    📈 3. Handling Outliers\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(111, 66, 193, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Identify and handle extreme values that can skew model performance.\n",
    "\n",
    "### **🔍 Detection Methods**\n",
    "\n",
    "#### **1️⃣ Visual Methods**\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Box plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "df.boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Individual box plots\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(f'Box Plot: {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plots\n",
    "sns.pairplot(df[numerical_cols])\n",
    "plt.show()\n",
    "\n",
    "# Histograms\n",
    "df[numerical_cols].hist(bins=20, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **2️⃣ Statistical Methods**\n",
    "\n",
    "**📐 IQR Method:** Values beyond Q1-1.5×IQR or Q3+1.5×IQR\n",
    "**📊 Z-Score:** Values with |z-score| > 3\n",
    "**🎯 Modified Z-Score:** Using median absolute deviation\n",
    "\n",
    "### **🛠️ Treatment Options**\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin: 15px 0;\">\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px;\">\n",
    "<strong>🗑️ Remove</strong><br>\n",
    "Delete outlier records\n",
    "</div>\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px;\">\n",
    "<strong>🔒 Cap/Floor</strong><br>\n",
    "Set to percentile limits\n",
    "</div>\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px;\">\n",
    "<strong>🔄 Transform</strong><br>\n",
    "Log, square root, Box-Cox\n",
    "</div>\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px;\">\n",
    "<strong>📦 Binning</strong><br>\n",
    "Convert to categorical ranges\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox\n",
    "import numpy as np\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(df[column]))\n",
    "    outliers = df[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Example: Handle outliers in a specific column\n",
    "column = 'price'  # Replace with your column name\n",
    "\n",
    "# Method 1: IQR Method\n",
    "outliers, lower, upper = detect_outliers_iqr(df, column)\n",
    "print(f\"Outliers detected: {len(outliers)}\")\n",
    "print(f\"Lower bound: {lower:.2f}, Upper bound: {upper:.2f}\")\n",
    "\n",
    "# Remove outliers\n",
    "df_clean = df[(df[column] >= lower) & (df[column] <= upper)]\n",
    "\n",
    "# Method 2: Winsorization (Cap/Floor)\n",
    "df[column] = np.clip(df[column], lower, upper)\n",
    "\n",
    "# Method 3: Z-Score Method\n",
    "z_scores = np.abs(stats.zscore(df[numerical_cols]))\n",
    "df_zscore = df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Method 4: Log transformation for skewed data\n",
    "df[f'{column}_log'] = np.log1p(df[column])  # log1p = log(1+x)\n",
    "\n",
    "# Method 5: Box-Cox transformation\n",
    "df[f'{column}_boxcox'], lambda_param = boxcox(df[column] + 1)  # +1 to handle zeros\n",
    "```\n",
    "\n",
    "<div style=\"background: rgba(255, 193, 7, 0.1); border-left: 4px solid #ffc107; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**⚠️ Important:** Don't automatically remove all outliers! Some might represent important patterns or rare but valid cases. Always investigate the context before deciding on treatment.\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #17a2b8, #6610f2); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🔄 4. Data Type Conversion\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(23, 162, 184, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Ensure data types are appropriate for analysis and modeling.\n",
    "\n",
    "### **🔄 Common Conversions**\n",
    "\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- Convert `object` to `category` for categorical data (saves memory)\n",
    "- Convert `strings` to `datetime` for temporal data\n",
    "- Convert `categorical text labels` to `numerical codes`\n",
    "- Convert `boolean strings` to `actual boolean type`\n",
    "\n",
    "</div>\n",
    "\n",
    "### **✅ Benefits**\n",
    "- ✅ **Improved memory efficiency**\n",
    "- ✅ **Better performance in operations**\n",
    "- ✅ **Enables appropriate statistical operations**\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Before optimization - check current memory usage\n",
    "print(\"Memory usage before optimization:\")\n",
    "print(df.info(memory_usage='deep'))\n",
    "\n",
    "# 1. Convert to category (saves memory for repeated strings)\n",
    "categorical_columns = ['gender', 'city', 'product_category']\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# 2. Convert to datetime\n",
    "date_columns = ['purchase_date', 'birth_date', 'registration_date']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# 3. Convert boolean strings\n",
    "boolean_mappings = {\n",
    "    'is_premium': {'True': True, 'False': False, 'true': True, 'false': False},\n",
    "    'is_active': {'Yes': True, 'No': False, 'Y': True, 'N': False}\n",
    "}\n",
    "\n",
    "for col, mapping in boolean_mappings.items():\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(mapping)\n",
    "\n",
    "# 4. Optimize numeric types\n",
    "# Downcast integers\n",
    "int_columns = df.select_dtypes(include=['int64']).columns\n",
    "for col in int_columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "# Downcast floats\n",
    "float_columns = df.select_dtypes(include=['float64']).columns\n",
    "for col in float_columns:\n",
    "    df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "# 5. Handle mixed data types\n",
    "def smart_convert(series):\n",
    "    \"\"\"Intelligently convert series to appropriate data type\"\"\"\n",
    "    # Try numeric first\n",
    "    try:\n",
    "        return pd.to_numeric(series)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try datetime\n",
    "    try:\n",
    "        return pd.to_datetime(series)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try boolean\n",
    "    if series.nunique() <= 2:\n",
    "        unique_vals = series.unique()\n",
    "        if set(unique_vals).issubset({'True', 'False', 'true', 'false', '1', '0', 1, 0}):\n",
    "            return series.map({'True': True, 'False': False, 'true': True, 'false': False, '1': True, '0': False, 1: True, 0: False})\n",
    "    \n",
    "    # Default to category if few unique values\n",
    "    if series.nunique() / len(series) < 0.5:\n",
    "        return series.astype('category')\n",
    "    \n",
    "    return series\n",
    "\n",
    "# Apply smart conversion to object columns\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in object_columns:\n",
    "    df[col] = smart_convert(df[col])\n",
    "\n",
    "# After optimization - check memory usage\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df.info(memory_usage='deep'))\n",
    "\n",
    "# Create a memory usage comparison function\n",
    "def compare_memory_usage(df_before, df_after):\n",
    "    \"\"\"Compare memory usage before and after optimization\"\"\"\n",
    "    memory_before = df_before.memory_usage(deep=True).sum()\n",
    "    memory_after = df_after.memory_usage(deep=True).sum()\n",
    "    reduction = (memory_before - memory_after) / memory_before * 100\n",
    "    \n",
    "    print(f\"Memory before: {memory_before / 1024**2:.2f} MB\")\n",
    "    print(f\"Memory after:  {memory_after / 1024**2:.2f} MB\")\n",
    "    print(f\"Reduction:     {reduction:.1f}%\")\n",
    "```\n",
    "\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); border-left: 4px solid #28a745; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**🚀 Pro Tip:** Proper data type conversion can reduce memory usage by 50-90% for large datasets, significantly improving processing speed!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #fd7e14, #dc3545); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🏷️ 5. Encoding Categorical Variables\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(253, 126, 20, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Convert categorical data into numerical format for machine learning algorithms.\n",
    "\n",
    "### **🎯 Encoding Methods**\n",
    "\n",
    "#### **1️⃣ Label Encoding**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Best for:** Ordinal features (with natural order)\n",
    "- **Creates:** Single column with integer values\n",
    "- **Example:** Education level (High School=0, Bachelor=1, Master=2, PhD=3)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ One-Hot Encoding**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Best for:** Nominal features (no natural order)\n",
    "- **Creates:** Multiple binary columns\n",
    "- **Example:** Color (Red, Blue, Green) → 3 binary columns\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Target/Mean Encoding**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Best for:** High cardinality categorical features\n",
    "- **Risk:** Data leakage if not done properly\n",
    "- **Use with:** Cross-validation and regularization\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **4️⃣ Binary Encoding**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Best for:** High cardinality features (more efficient than one-hot)\n",
    "- **Creates:** Log₂(n) binary columns\n",
    "\n",
    "</div>\n",
    "\n",
    "### **📦 Installation**\n",
    "```bash\n",
    "pip install category_encoders\n",
    "```\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor'],\n",
    "    'color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC', 'Boston'],\n",
    "    'target': [0, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# 1. Label Encoding (for ordinal data)\n",
    "le = LabelEncoder()\n",
    "education_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "df['education_encoded'] = df['education'].map({v: i for i, v in enumerate(education_order)})\n",
    "\n",
    "# Alternative using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['education_le'] = le.fit_transform(df['education'])\n",
    "print(\"Label Encoding mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# 2. One-Hot Encoding\n",
    "# Method 1: Using pandas get_dummies\n",
    "df_onehot = pd.get_dummies(df, columns=['color'], prefix='color', drop_first=True)\n",
    "\n",
    "# Method 2: Using sklearn OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False, drop='first')\n",
    "color_encoded = ohe.fit_transform(df[['color']])\n",
    "color_columns = [f'color_{cat}' for cat in ohe.categories_[0][1:]]  # Skip first due to drop='first'\n",
    "df_ohe = pd.concat([df, pd.DataFrame(color_encoded, columns=color_columns)], axis=1)\n",
    "\n",
    "# 3. Target Encoding (use with caution - potential data leakage)\n",
    "target_encoder = ce.TargetEncoder()\n",
    "df['city_target_encoded'] = target_encoder.fit_transform(df['city'], df['target'])\n",
    "\n",
    "# 4. Binary Encoding (for high cardinality)\n",
    "binary_encoder = ce.BinaryEncoder()\n",
    "df_binary = binary_encoder.fit_transform(df['city'])\n",
    "df = pd.concat([df, df_binary], axis=1)\n",
    "\n",
    "# 5. Advanced: Frequency Encoding\n",
    "def frequency_encoding(series):\n",
    "    \"\"\"Encode categories by their frequency\"\"\"\n",
    "    frequency_map = series.value_counts().to_dict()\n",
    "    return series.map(frequency_map)\n",
    "\n",
    "df['city_frequency'] = frequency_encoding(df['city'])\n",
    "\n",
    "# 6. Advanced: Mean Encoding with Cross-Validation (safer)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def safe_target_encoding(X, y, column, n_splits=5):\n",
    "    \"\"\"Target encoding with cross-validation to prevent leakage\"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    encoded = np.zeros(len(X))\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        train_mean = y.iloc[train_idx].groupby(X[column].iloc[train_idx]).mean()\n",
    "        encoded[val_idx] = X[column].iloc[val_idx].map(train_mean)\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# Example usage\n",
    "df['city_safe_target'] = safe_target_encoding(df, df['target'], 'city')\n",
    "\n",
    "print(\"Encoding Results:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### **📊 Comparison Table**\n",
    "\n",
    "| Method | Use Case | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Label Encoding** | Ordinal data | Simple, memory efficient | Assumes order, may mislead algorithms |\n",
    "| **One-Hot Encoding** | Nominal data (low cardinality) | No false relationships | High dimensionality, sparse matrices |\n",
    "| **Target Encoding** | High cardinality | Compact, captures target relationship | Risk of overfitting, leakage |\n",
    "| **Binary Encoding** | High cardinality | More compact than one-hot | Less interpretable |\n",
    "\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**⚠️ Critical:** Always apply the same encoding transformations to both training and test data using the same fitted encoder to avoid data leakage!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #20c997, #28a745); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    ⚖️ 6. Feature Scaling\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(32, 201, 151, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Normalize feature ranges to prevent algorithms from being biased toward features with larger scales.\n",
    "\n",
    "### **🎯 When Needed**\n",
    "<div style=\"background: rgba(32, 201, 151, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "**✅ Scaling Required:**\n",
    "- **Distance-based algorithms:** KNN, K-Means, SVM\n",
    "- **Gradient-based algorithms:** Neural Networks, Logistic Regression\n",
    "- **Regularized algorithms:** Ridge, Lasso, Elastic Net\n",
    "\n",
    "**❌ Scaling NOT Needed:**\n",
    "- **Tree-based algorithms:** Random Forest, Decision Trees, XGBoost\n",
    "\n",
    "</div>\n",
    "\n",
    "### **📏 Scaling Methods**\n",
    "\n",
    "#### **1️⃣ MinMaxScaler**\n",
    "- **Range:** [0, 1]\n",
    "- **Formula:** (x - min) / (max - min)\n",
    "- **Best for:** Bounded data, when you know min/max\n",
    "\n",
    "#### **2️⃣ StandardScaler (Z-score)**\n",
    "- **Range:** Mean=0, Std=1\n",
    "- **Formula:** (x - mean) / std\n",
    "- **Best for:** Normally distributed data\n",
    "\n",
    "#### **3️⃣ RobustScaler**\n",
    "- **Uses:** Median and IQR instead of mean and std\n",
    "- **Best for:** Data with outliers\n",
    "\n",
    "### **⚠️ Critical Rule**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "When using **MinMaxScaler** or **StandardScaler**, you should use `fit` on **train data only** and use `transform` on **test data** to prevent data leakage!\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data preparation\n",
    "X, y = df.drop('target', axis=1), df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select numerical columns for scaling\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 1. MinMax Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(X_train[numerical_cols])  # Fit only on training data\n",
    "\n",
    "X_train_minmax = X_train.copy()\n",
    "X_test_minmax = X_test.copy()\n",
    "X_train_minmax[numerical_cols] = minmax_scaler.transform(X_train[numerical_cols])\n",
    "X_test_minmax[numerical_cols] = minmax_scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# 2. Standard Scaling\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "X_train_std = X_train.copy()\n",
    "X_test_std = X_test.copy()\n",
    "X_train_std[numerical_cols] = std_scaler.transform(X_train[numerical_cols])\n",
    "X_test_std[numerical_cols] = std_scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# 3. Robust Scaling\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X_train[numerical_cols])\n",
    "\n",
    "X_train_robust = X_train.copy()\n",
    "X_test_robust = X_test.copy()\n",
    "X_train_robust[numerical_cols] = robust_scaler.transform(X_train[numerical_cols])\n",
    "X_test_robust[numerical_cols] = robust_scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Visualization function\n",
    "def plot_scaling_comparison(original, minmax, standard, robust, feature_name):\n",
    "    \"\"\"Compare different scaling methods visually\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Original\n",
    "    axes[0,0].hist(original, bins=30, alpha=0.7)\n",
    "    axes[0,0].set_title(f'Original {feature_name}')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # MinMax\n",
    "    axes[0,1].hist(minmax, bins=30, alpha=0.7, color='orange')\n",
    "    axes[0,1].set_title(f'MinMax Scaled {feature_name}')\n",
    "    \n",
    "    # Standard\n",
    "    axes[1,0].hist(standard, bins=30, alpha=0.7, color='green')\n",
    "    axes[1,0].set_title(f'Standard Scaled {feature_name}')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Robust\n",
    "    axes[1,1].hist(robust, bins=30, alpha=0.7, color='red')\n",
    "    axes[1,1].set_title(f'Robust Scaled {feature_name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example visualization (replace 'feature_name' with actual column)\n",
    "if len(numerical_cols) > 0:\n",
    "    feature = numerical_cols[0]\n",
    "    plot_scaling_comparison(\n",
    "        X_train[feature],\n",
    "        X_train_minmax[feature],\n",
    "        X_train_std[feature],\n",
    "        X_train_robust[feature],\n",
    "        feature\n",
    "    )\n",
    "\n",
    "# Statistical comparison\n",
    "def scaling_stats(original, scaled, method_name):\n",
    "    \"\"\"Print statistics for scaled data\"\"\"\n",
    "    print(f\"\\n{method_name} Scaling Statistics:\")\n",
    "    print(f\"Original - Mean: {original.mean():.3f}, Std: {original.std():.3f}\")\n",
    "    print(f\"Scaled   - Mean: {scaled.mean():.3f}, Std: {scaled.std():.3f}\")\n",
    "    print(f\"Scaled   - Min: {scaled.min():.3f}, Max: {scaled.max():.3f}\")\n",
    "\n",
    "# Compare all scaling methods\n",
    "for feature in numerical_cols[:1]:  # Compare first numerical feature\n",
    "    scaling_stats(X_train[feature], X_train_minmax[feature], \"MinMax\")\n",
    "    scaling_stats(X_train[feature], X_train_std[feature], \"Standard\")\n",
    "    scaling_stats(X_train[feature], X_train_robust[feature], \"Robust\")\n",
    "```\n",
    "\n",
    "### **🎯 Choosing the Right Scaler**\n",
    "\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "| **Scaler** | **Best For** | **Pros** | **Cons** |\n",
    "|------------|-------------|----------|----------|\n",
    "| **MinMaxScaler** | Bounded features, uniform distribution | Fixed range [0,1], preserves relationships | Sensitive to outliers |\n",
    "| **StandardScaler** | Normal distribution, no outliers | Centers data, unit variance | Assumes normal distribution |\n",
    "| **RobustScaler** | Features with outliers | Robust to outliers | May not scale to [0,1] |\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**⚠️ Remember:** Always fit the scaler on training data only, then transform both training and test data to prevent data leakage!\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #fd7e14, #e83e8c); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🛠️ 7. Feature Engineering\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(253, 126, 20, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Create new features from existing ones to improve model performance and capture domain knowledge.\n",
    "\n",
    "### **🎯 Common Techniques**\n",
    "\n",
    "#### **1️⃣ Mathematical Operations**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Ratios:** income/expense, price/sqft\n",
    "- **Differences:** current_price - previous_price  \n",
    "- **Products:** length × width for area\n",
    "- **Powers:** x², √x for non-linear relationships\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Date/Time Features**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Extract:** year, month, day, hour, day_of_week\n",
    "- **Create:** is_weekend, is_holiday, days_since_event\n",
    "- **Cyclical:** sin/cos transformations for hours, months\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Text Features**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Length:** character count, word count\n",
    "- **Patterns:** email domains, phone area codes\n",
    "- **Sentiment:** positive/negative scoring\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **4️⃣ Binning/Discretization**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Age groups:** 0-18, 19-35, 36-50, 50+\n",
    "- **Income brackets:** Low, Medium, High\n",
    "- **Performance tiers:** A, B, C grades\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **5️⃣ Polynomial Features**\n",
    "<div style=\"background: rgba(253, 126, 20, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Powers:** x², x³ for non-linear patterns\n",
    "- **Interactions:** x₁×x₂ for feature combinations\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔧 Advanced Tools**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); border-left: 4px solid #17a2b8; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "👍 You can also use **Featuretools** for automatic feature engineering\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Date feature engineering\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['is_weekend'] = df['date'].dt.dayofweek >= 5\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "\n",
    "# Cyclical features for time\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Mathematical operations\n",
    "df['bmi'] = df['weight'] / (df['height'] ** 2)\n",
    "df['price_per_sqft'] = df['price'] / df['area']\n",
    "df['income_expense_ratio'] = df['income'] / df['expense']\n",
    "\n",
    "# Text features\n",
    "df['text_length'] = df['description'].str.len()\n",
    "df['word_count'] = df['description'].str.split().str.len()\n",
    "df['email_domain'] = df['email'].str.split('@').str[1]\n",
    "\n",
    "# Binning\n",
    "df['age_group'] = pd.cut(df['age'], \n",
    "                        bins=[0, 18, 35, 50, 100], \n",
    "                        labels=['Child', 'Young', 'Adult', 'Senior'])\n",
    "\n",
    "df['income_bracket'] = pd.qcut(df['income'], \n",
    "                              q=3, \n",
    "                              labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['feature1', 'feature2']])\n",
    "poly_feature_names = poly.get_feature_names_out(['feature1', 'feature2'])\n",
    "\n",
    "# Create polynomial dataframe\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
    "```\n",
    "\n",
    "### **💡 Feature Engineering Tips**\n",
    "\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); border-left: 4px solid #28a745; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "**✅ Best Practices:**\n",
    "- **Domain Knowledge:** Use business understanding to create meaningful features\n",
    "- **Validation:** Always validate new features improve model performance  \n",
    "- **Correlation Check:** Remove highly correlated engineered features\n",
    "- **Feature Importance:** Use tree-based models to identify valuable features\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #6f42c1, #e83e8c); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    🎯 8. Feature Selection\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(111, 66, 193, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Select the most relevant features to improve model performance and reduce overfitting.\n",
    "\n",
    "### **✅ Benefits**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- ✅ **Reduces overfitting**\n",
    "- ✅ **Improves model interpretability**  \n",
    "- ✅ **Decreases training time**\n",
    "- ✅ **Reduces storage requirements**\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔍 Selection Methods**\n",
    "\n",
    "#### **1️⃣ Filter Methods (Statistical)**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Correlation Matrix:** Remove highly correlated features (>0.95)\n",
    "- **Chi-square Test:** For categorical features vs categorical target\n",
    "- **ANOVA F-test:** For numerical features vs categorical target  \n",
    "- **Mutual Information:** Measures dependency between features and target\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Wrapper Methods**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Recursive Feature Elimination (RFE):** Iteratively remove features\n",
    "- **Forward/Backward Selection:** Add/remove features stepwise\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Embedded Methods**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **L1 Regularization (Lasso):** Automatically selects features\n",
    "- **Tree-based Feature Importance:** From Random Forest, XGBoost\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔧 Advanced Tip**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); border-left: 4px solid #17a2b8; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "You can also use **SelectFromModel** with **Lasso** for automatic feature selection\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Correlation Analysis\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Remove highly correlated features\n",
    "def remove_correlated_features(df, threshold=0.95):\n",
    "    \"\"\"Remove features with correlation > threshold\"\"\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    high_corr_features = [column for column in upper_triangle.columns \n",
    "                         if any(upper_triangle[column] > threshold)]\n",
    "    \n",
    "    return df.drop(columns=high_corr_features), high_corr_features\n",
    "\n",
    "df_reduced, removed_features = remove_correlated_features(df[numerical_cols])\n",
    "print(f\"Removed highly correlated features: {removed_features}\")\n",
    "\n",
    "# 2. Chi-square for categorical features\n",
    "chi2_selector = SelectKBest(chi2, k=10)\n",
    "X_chi2 = chi2_selector.fit_transform(X_categorical, y)\n",
    "chi2_scores = chi2_selector.scores_\n",
    "chi2_features = X_categorical.columns[chi2_selector.get_support()]\n",
    "\n",
    "print(\"Top Chi-square features:\")\n",
    "feature_scores = list(zip(chi2_features, chi2_scores[chi2_selector.get_support()]))\n",
    "for feature, score in sorted(feature_scores, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{feature}: {score:.2f}\")\n",
    "\n",
    "# 3. ANOVA F-test for numerical features\n",
    "f_selector = SelectKBest(f_classif, k=10)\n",
    "X_f = f_selector.fit_transform(X_numerical, y)\n",
    "f_scores = f_selector.scores_\n",
    "f_features = X_numerical.columns[f_selector.get_support()]\n",
    "\n",
    "# 4. RFE with Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfe = RFE(estimator=rf, n_features_to_select=10, step=1)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "selected_features_rfe = X.columns[rfe.support_]\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "print(\"RFE Selected Features:\")\n",
    "for i, feature in enumerate(selected_features_rfe):\n",
    "    print(f\"{feature}: Rank {feature_ranking[X.columns.get_loc(feature)]}\")\n",
    "\n",
    "# 5. Lasso Feature Selection\n",
    "lasso = Lasso(alpha=0.01, random_state=42)\n",
    "lasso_selector = SelectFromModel(lasso)\n",
    "X_lasso = lasso_selector.fit_transform(X, y)\n",
    "\n",
    "selected_features_lasso = X.columns[lasso_selector.get_support()]\n",
    "print(f\"Lasso selected {len(selected_features_lasso)} features:\")\n",
    "print(selected_features_lasso.tolist())\n",
    "\n",
    "# 6. Feature Importance from Random Forest\n",
    "rf.fit(X, y)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "sns.barplot(data=top_features, y='feature', x='importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **📊 Feature Selection Comparison**\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "| **Method** | **Type** | **Best For** | **Pros** | **Cons** |\n",
    "|------------|----------|--------------|----------|----------|\n",
    "| **Correlation** | Filter | Linear relationships | Fast, simple | Misses non-linear relationships |\n",
    "| **Chi-square** | Filter | Categorical features | Statistical significance | Only for categorical |\n",
    "| **ANOVA F-test** | Filter | Numerical features | Statistical foundation | Assumes normal distribution |\n",
    "| **RFE** | Wrapper | Any algorithm | Algorithm-specific | Computationally expensive |\n",
    "| **Lasso** | Embedded | Linear models | Automatic selection | Linear assumptions |\n",
    "| **Tree Importance** | Embedded | Tree models | Handles non-linearity | Model-specific |\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #17a2b8, #6610f2); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    📝 9. Text Preprocessing (if applicable)\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(23, 162, 184, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Clean and prepare text data for NLP and machine learning tasks.\n",
    "\n",
    "### **🔄 Common Steps**\n",
    "\n",
    "#### **1️⃣ Basic Cleaning**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Lowercasing:** Convert all text to lowercase\n",
    "- **Remove punctuation:** Clean special characters  \n",
    "- **Remove extra whitespace:** Handle spacing issues\n",
    "- **Handle encoding:** Fix UTF-8, ASCII issues\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Tokenization**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Split text:** Into individual words/tokens\n",
    "- **Handle contractions:** don't → do not\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Stopwords Removal**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Remove common words:** the, and, or, etc.\n",
    "- **Language-specific:** Use appropriate stopword lists\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **4️⃣ Normalization**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Stemming:** Reduce words to root form (running → run)\n",
    "- **Lemmatization:** Reduce to dictionary form (better → good)\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **5️⃣ Vectorization**\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Bag of Words:** Count frequency of words\n",
    "- **TF-IDF:** Term frequency-inverse document frequency  \n",
    "- **Word Embeddings:** Word2Vec, GloVe, FastText\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔧 Advanced Tools**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); border-left: 4px solid #28a745; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "👉 You can also use **Contractions** for fixing contractions in text  \n",
    "👉 You can also use **spaCy** for more advanced text processing\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, use_lemmatizer=False):\n",
    "    \"\"\"Comprehensive text preprocessing function\"\"\"\n",
    "    # Handle contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, and special patterns\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords and apply stemming/lemmatization\n",
    "    if use_lemmatizer:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens \n",
    "                 if word not in stop_words and len(word) > 2]\n",
    "    else:\n",
    "        tokens = [stemmer.stem(word) for word in tokens \n",
    "                 if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_text'] = df['text'].apply(lambda x: preprocess_text(x, use_lemmatizer=True))\n",
    "\n",
    "# Advanced preprocessing with spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    \"\"\"Advanced preprocessing using spaCy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract lemmatized tokens, remove stop words and punctuation\n",
    "    tokens = [token.lemma_.lower() for token in doc \n",
    "             if not token.is_stop and not token.is_punct \n",
    "             and not token.is_space and len(token.text) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['spacy_cleaned'] = df['text'].apply(spacy_preprocess)\n",
    "\n",
    "# Vectorization\n",
    "# 1. Bag of Words\n",
    "bow_vectorizer = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "bow_features = bow_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), \n",
    "                                  min_df=2, max_df=0.95)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Get feature names\n",
    "bow_feature_names = bow_vectorizer.get_feature_names_out()\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Bag of Words features: {bow_features.shape}\")\n",
    "print(f\"TF-IDF features: {tfidf_features.shape}\")\n",
    "\n",
    "# Feature analysis\n",
    "def analyze_text_features(vectorizer, features, feature_names, top_n=10):\n",
    "    \"\"\"Analyze most important text features\"\"\"\n",
    "    # Sum features across all documents\n",
    "    feature_sums = np.array(features.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_sums\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"Top {top_n} most frequent features:\")\n",
    "    print(feature_importance.head(top_n))\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Analyze features\n",
    "bow_importance = analyze_text_features(bow_vectorizer, bow_features, bow_feature_names)\n",
    "tfidf_importance = analyze_text_features(tfidf_vectorizer, tfidf_features, tfidf_feature_names)\n",
    "```\n",
    "\n",
    "### **📊 Text Preprocessing Comparison**\n",
    "\n",
    "<div style=\"background: rgba(23, 162, 184, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "| **Method** | **Purpose** | **Pros** | **Cons** |\n",
    "|------------|-------------|----------|----------|\n",
    "| **Stemming** | Root form reduction | Fast, simple | Can be aggressive, lose meaning |\n",
    "| **Lemmatization** | Dictionary form | Maintains meaning | Slower, requires POS tags |\n",
    "| **Bag of Words** | Word frequency | Simple, interpretable | Loses word order, sparse |\n",
    "| **TF-IDF** | Weighted frequency | Reduces common word impact | Still loses context |\n",
    "| **Word Embeddings** | Dense representations | Captures semantics | Requires pre-training |\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #dc3545, #fd7e14); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    ✂️ 10. Data Splitting\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(220, 53, 69, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Separate data into training and testing sets to evaluate model performance on unseen data.\n",
    "\n",
    "### **📊 Common Split Ratios**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **80/20** (Train/Test)\n",
    "- **70/30** (Train/Test)  \n",
    "- **60/20/20** (Train/Validation/Test)\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔄 Types of Splitting**\n",
    "\n",
    "#### **1️⃣ Random Split**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Good for:** Independent observations\n",
    "- **Use:** `train_test_split()`\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Stratified Split**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Good for:** Imbalanced datasets\n",
    "- **Maintains:** Class distribution in both sets\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Time-based Split**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Good for:** Time series data\n",
    "- **Rule:** Train on past, test on future\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **4️⃣ Cross-validation**\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **K-fold:** Split data into k folds\n",
    "- **Stratified K-fold:** Maintains class distribution\n",
    "- **Time series split:** Respects temporal order\n",
    "\n",
    "</div>\n",
    "\n",
    "### **⚠️ Important Note**\n",
    "<div style=\"background: rgba(255, 193, 7, 0.1); border-left: 4px solid #ffc107; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "Use `cross_val_score` or `cross_validate` for model evaluation across folds.\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, \n",
    "                                   TimeSeriesSplit, cross_val_score, \n",
    "                                   GridSearchCV)\n",
    "import numpy as np\n",
    "\n",
    "# 1. Basic random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# 2. Stratified split for imbalanced data\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Original distribution:\", np.bincount(y) / len(y))\n",
    "print(\"Training distribution:\", np.bincount(y_train_strat) / len(y_train_strat))\n",
    "print(\"Test distribution:\", np.bincount(y_test_strat) / len(y_test_strat))\n",
    "\n",
    "# 3. Three-way split (Train/Validation/Test)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "print(f\"Training: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# 4. Time series split\n",
    "if 'date' in df.columns:\n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values('date')\n",
    "    \n",
    "    # Manual time-based split\n",
    "    train_size = int(0.8 * len(df_sorted))\n",
    "    train_data = df_sorted[:train_size]\n",
    "    test_data = df_sorted[train_size:]\n",
    "    \n",
    "    print(f\"Training period: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "    print(f\"Test period: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "\n",
    "# Time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    print(f\"Fold {fold+1}: Train size={len(X_train_fold)}, Test size={len(X_test_fold)}\")\n",
    "\n",
    "# 5. K-Fold Cross-Validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Standard K-Fold\n",
    "cv_scores = cross_val_score(RandomForestClassifier(random_state=42), \n",
    "                           X, y, cv=5, scoring='accuracy')\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "strat_scores = cross_val_score(RandomForestClassifier(random_state=42), \n",
    "                              X, y, cv=skf, scoring='accuracy')\n",
    "print(f\"Stratified CV Accuracy: {strat_scores.mean():.3f} (+/- {strat_scores.std() * 2:.3f})\")\n",
    "\n",
    "# 6. Cross-validation with multiple metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "cv_results = cross_validate(RandomForestClassifier(random_state=42), \n",
    "                           X, y, cv=5, scoring=scoring)\n",
    "\n",
    "for metric in scoring:\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.upper()}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "```\n",
    "\n",
    "### **📊 Cross-Validation Strategies**\n",
    "\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "| **Strategy** | **Use Case** | **Pros** | **Cons** |\n",
    "|--------------|--------------|----------|----------|\n",
    "| **K-Fold** | General purpose | Robust estimates | May not preserve class distribution |\n",
    "| **Stratified K-Fold** | Imbalanced data | Preserves class ratios | Requires categorical target |\n",
    "| **Time Series Split** | Temporal data | Respects time order | Smaller training sets in early folds |\n",
    "| **Leave-One-Out** | Small datasets | Maximum training data | Computationally expensive |\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.4em; font-weight: bold; cursor: pointer; padding: 10px; background: linear-gradient(45deg, #28a745, #20c997); color: white; border-radius: 8px; margin: 10px 0;\">\n",
    "    ⚖️ 11. Balancing the Dataset (if needed)\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 20px; background: rgba(40, 167, 69, 0.05); border-radius: 10px; margin: 10px 0;\">\n",
    "\n",
    "### **Purpose**\n",
    "Address class imbalance that can lead to biased model predictions.\n",
    "\n",
    "### **🎯 When to Use**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- 👉 **Imbalanced classification problems**\n",
    "- 👉 **Minority class < 10–20% of total data**\n",
    "- 👉 **When accuracy alone is not sufficient** (e.g., fraud detection, medical diagnosis)\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔄 Techniques**\n",
    "\n",
    "#### **1️⃣ Oversampling**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **SMOTE** (Synthetic Minority Oversampling Technique): Creates synthetic samples from the minority class\n",
    "- **ADASYN**: Adaptive version of SMOTE, focuses more on difficult examples\n",
    "- **Random Oversampling**: Duplicates existing samples from the minority class\n",
    "- **BorderlineSMOTE**: Oversamples near the decision boundary\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **2️⃣ Undersampling**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Random Undersampling**: Removes samples from the majority class\n",
    "- **Tomek Links**: Removes majority samples that are borderline\n",
    "- **Edited Nearest Neighbors**: Removes noisy or ambiguous samples\n",
    "\n",
    "</div>\n",
    "\n",
    "#### **3️⃣ Algorithmic Approaches**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Class Weights**: Increase penalty for misclassifying minority class\n",
    "- **Cost-sensitive Learning**: Custom loss functions for imbalance\n",
    "- **Ensemble Methods**: Use balanced subsets in ensemble models (e.g., BalancedRandomForest)\n",
    "\n",
    "</div>\n",
    "\n",
    "### **🔄 Avoiding Data Leakage with imblearn.pipeline**\n",
    "\n",
    "<div style=\"background: rgba(220, 53, 69, 0.1); border-left: 4px solid #dc3545; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "When oversampling is done **before** splitting data (train/test), it leaks information from the test set into training.  \n",
    "To prevent this, use `imblearn.pipeline.Pipeline` to perform oversampling **inside the cross-validation loop**:\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Implementation**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Check original distribution\n",
    "print(\"Original distribution:\", Counter(y))\n",
    "\n",
    "# 1. SMOTE Oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "print(\"After SMOTE:\", Counter(y_smote))\n",
    "\n",
    "# 2. ADASYN (Adaptive Synthetic Sampling)\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "print(\"After ADASYN:\", Counter(y_adasyn))\n",
    "\n",
    "# 3. Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "print(\"After Random Oversampling:\", Counter(y_ros))\n",
    "\n",
    "# 4. Borderline SMOTE\n",
    "borderline_smote = BorderlineSMOTE(random_state=42)\n",
    "X_borderline, y_borderline = borderline_smote.fit_resample(X, y)\n",
    "print(\"After Borderline SMOTE:\", Counter(y_borderline))\n",
    "\n",
    "# 5. Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "print(\"After Random Undersampling:\", Counter(y_rus))\n",
    "\n",
    "# 6. Tomek Links\n",
    "tomek = TomekLinks()\n",
    "X_tomek, y_tomek = tomek.fit_resample(X, y)\n",
    "print(\"After Tomek Links:\", Counter(y_tomek))\n",
    "\n",
    "# 7. Safe Pipeline with Cross-Validation (RECOMMENDED)\n",
    "# Build pipeline with oversampling and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validation with safe oversampling inside folds\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Pipeline F1 Scores: {scores}\")\n",
    "print(f\"Average F1: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "\n",
    "# 8. Class Weights Approach (Alternative to sampling)\n",
    "# This doesn't change the data size, just adjusts algorithm behavior\n",
    "rf_balanced = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "balanced_scores = cross_val_score(rf_balanced, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Balanced RF F1: {balanced_scores.mean():.3f} (+/- {balanced_scores.std() * 2:.3f})\")\n",
    "\n",
    "# 9. Custom class weights\n",
    "class_weights = {0: 1, 1: 10}  # Give 10x weight to minority class\n",
    "rf_custom = RandomForestClassifier(class_weight=class_weights, random_state=42)\n",
    "custom_scores = cross_val_score(rf_custom, X, y, cv=5, scoring='f1_macro')\n",
    "print(f\"Custom Weighted RF F1: {custom_scores.mean():.3f} (+/- {custom_scores.std() * 2:.3f})\")\n",
    "\n",
    "# 10. Comparison of methods\n",
    "methods = {\n",
    "    'Original': (X, y),\n",
    "    'SMOTE': (X_smote, y_smote),\n",
    "    'ADASYN': (X_adasyn, y_adasyn),\n",
    "    'Random Oversample': (X_ros, y_ros),\n",
    "    'Random Undersample': (X_rus, y_rus)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (X_method, y_method) in methods.items():\n",
    "    if name == 'Original':\n",
    "        clf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    scores = cross_val_score(clf, X_method, y_method, cv=5, scoring='f1_macro')\n",
    "    results[name] = scores.mean()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nMethod Comparison (F1-Score):\")\n",
    "for method, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{method:20}: {score:.3f}\")\n",
    "```\n",
    "\n",
    "### **📊 Sampling Methods Comparison**\n",
    "\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "| **Method** | **Type** | **Pros** | **Cons** |\n",
    "|------------|----------|----------|----------|\n",
    "| **SMOTE** | Oversample | Creates realistic synthetic samples | May create noise in complex datasets |\n",
    "| **ADASYN** | Oversample | Focuses on difficult cases | Can increase class overlap |\n",
    "| **Random Oversample** | Oversample | Simple, preserves all information | Risk of overfitting |\n",
    "| **Random Undersample** | Undersample | Reduces training time | Loss of potentially useful information |\n",
    "| **Tomek Links** | Undersample | Removes borderline cases | May remove useful boundary information |\n",
    "| **Class Weights** | Algorithmic | No data modification | Algorithm-dependent effectiveness |\n",
    "\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Evaluation Metrics for Imbalanced Data**\n",
    "\n",
    "<details>\n",
    "<summary style=\"font-size: 1.2em; font-weight: bold; cursor: pointer; padding: 8px; background: rgba(108, 117, 125, 0.1); border-radius: 6px; margin: 8px 0;\">\n",
    "    Click to expand evaluation metrics\n",
    "</summary>\n",
    "\n",
    "<div style=\"padding: 15px; background: rgba(108, 117, 125, 0.05); border-radius: 8px; margin: 10px 0;\">\n",
    "\n",
    "### **Beyond Accuracy**\n",
    "<div style=\"background: rgba(108, 117, 125, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **Precision**: $TP / (TP + FP)$ - How many positive predictions were correct?\n",
    "- **Recall (Sensitivity)**: $TP / (TP + FN)$ - How many actual positives were found?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **ROC-AUC**: Area under the receiver operating characteristic curve\n",
    "- **PR-AUC**: Area under the precision-recall curve (better for imbalanced data)\n",
    "\n",
    "</div>\n",
    "\n",
    "### **✅ Key Recommendation**\n",
    "<div style=\"background: rgba(40, 167, 69, 0.1); border-left: 4px solid #28a745; padding: 15px; margin: 15px 0;\">\n",
    "\n",
    "Use **PR-AUC** instead of **ROC-AUC** when the dataset is **highly imbalanced**.\n",
    "\n",
    "</div>\n",
    "\n",
    "### **💻 Code Example**\n",
    "```python\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, average_precision_score,\n",
    "                           precision_recall_curve, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "print(f\"PR-AUC: {average_precision_score(y_test, y_pred_proba):.3f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_proba):.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "axes[1].plot(recall, precision, label=f'PR Curve (AUC = {average_precision_score(y_test, y_pred_proba):.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 **Summary Workflow Diagram**\n",
    "\n",
    "<div style=\"background: rgba(0,123,255,0.1); \n",
    "            border-left: 4px solid #007bff; \n",
    "            padding: 20px; \n",
    "            margin: 20px 0; \n",
    "            border-radius: 0 10px 10px 0;\n",
    "            font-family: 'Courier New', monospace;\">\n",
    "\n",
    "```\n",
    "📊 Data Loading & Understanding\n",
    "           ↓\n",
    "🔍 Missing Values Detection & Handling\n",
    "           ↓\n",
    "📈 Outlier Detection & Treatment\n",
    "           ↓\n",
    "🔄 Data Type Conversion & Optimization\n",
    "           ↓\n",
    "🏷️ Categorical Variable Encoding\n",
    "           ↓\n",
    "⚖️ Feature Scaling & Normalization\n",
    "           ↓\n",
    "🛠️ Feature Engineering & Creation\n",
    "           ↓\n",
    "🎯 Feature Selection & Reduction\n",
    "           ↓\n",
    "📝 Text Preprocessing (if applicable)\n",
    "           ↓\n",
    "✂️ Data Splitting (Train/Validation/Test)\n",
    "           ↓\n",
    "⚖️ Class Balancing (if needed)\n",
    "           ↓\n",
    "🤖 Model Training & Evaluation\n",
    "           ↓\n",
    "🔁 Iterative Improvement & Hyperparameter Tuning\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 **Useful Libraries & Resources**\n",
    "\n",
    "<div style=\"background: rgba(111, 66, 193, 0.05); \n",
    "            padding: 20px; \n",
    "            border-radius: 10px; \n",
    "            margin: 20px 0;\">\n",
    "\n",
    "### **Essential Libraries**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **`pandas`** - Data manipulation and analysis\n",
    "- **`numpy`** - Numerical computing  \n",
    "- **`scikit-learn`** - Machine learning preprocessing and algorithms\n",
    "- **`matplotlib` & `seaborn`** - Data visualization\n",
    "- **`imbalanced-learn`** - Handling imbalanced datasets\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Advanced Libraries**\n",
    "<div style=\"background: rgba(111, 66, 193, 0.1); padding: 15px; border-radius: 8px; margin: 15px 0;\">\n",
    "\n",
    "- **`feature-engine`** - Advanced feature engineering\n",
    "- **`category_encoders`** - Specialized categorical encoding\n",
    "- **`missingno`** - Missing data visualization  \n",
    "- **`yellowbrick`** - Machine learning visualization\n",
    "- **`featuretools`** - Automated feature engineering\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Installation Commands**\n",
    "```bash\n",
    "# Essential packages\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "# Imbalanced data\n",
    "pip install imbalanced-learn\n",
    "\n",
    "# Advanced preprocessing\n",
    "pip install feature-engine category_encoders missingno\n",
    "\n",
    "# Text processing\n",
    "pip install nltk spacy contractions\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "# Visualization\n",
    "pip install yellowbrick plotly\n",
    "\n",
    "# Automated feature engineering\n",
    "pip install featuretools\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background: rgba(255, 193, 7, 0.1); \n",
    "            border: 2px solid #ffc107; \n",
    "            padding: 20px; \n",
    "            border-radius: 15px; \n",
    "            margin: 25px 0; \n",
    "            text-align: center;\">\n",
    "\n",
    "### **🤔 Remember**\n",
    "\n",
    "**The best preprocessing pipeline depends on your specific dataset, problem type, and chosen algorithms. Always validate your preprocessing decisions with domain expertise and cross-validation!**\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, rgb(37, 127, 201), #8b000e); \n",
    "            color: #ffffff; \n",
    "            width: 100%; \n",
    "            height: 50px; \n",
    "            text-align: center; \n",
    "            font-weight: bold; \n",
    "            line-height: 50px; \n",
    "            margin: 25px 0; \n",
    "            font-size: 24px; \n",
    "            border-radius: 15px; \n",
    "            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);\">\n",
    "    By Abdelrhman Ezzat 🫡\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
