{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import json\n",
    "\n",
    "# Scikit-learn: Core ML utilities\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.imputers = {}\n",
    "        self.encoders = {}\n",
    "        self.feature_names = []\n",
    "        self.steps_log = []             # to record every preprocessing step as  writing down in a notepad\n",
    "        self.balancing_info = {}        # to record info in balance(eg. number of samples before and after SMOTE or undersampling).\n",
    "    \n",
    "    def log_step(self, step_name: str, details: str):\n",
    "        \"\"\"Log preprocessing steps for tracking.\"\"\"\n",
    "        log_entry = f\"✅ {step_name}: {details}\"\n",
    "        self.steps_log.append(log_entry)\n",
    "        print(log_entry)\n",
    "    # =================== 1. DATA UNDERSTANDING & INSPECTION ===================\n",
    "    def data_overview(self, df, sample_size= 5):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"Input 'df' must be a pandas DataFrame\")\n",
    "        if sample_size < 0:\n",
    "            raise ValueError(\"sample_size must be non-negative\")\n",
    "        \n",
    "        print(f\"{'=' * 20} 📊 DATA OVERVIEW {'=' * 20}\")\n",
    "        \n",
    "        # Compute overview metrics\n",
    "        try:\n",
    "            overview = {\n",
    "                'shape'          : df.shape,\n",
    "                'duplicated_rows': df.duplicated().sum(),\n",
    "                'missing_values' : df.isnull().sum().sum(),\n",
    "                'data_types'     : df.dtypes.value_counts().to_dict()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error computing overview metrics: {e}\")\n",
    "        \n",
    "        # Display overview metrics\n",
    "        print(f\"Dataset Shape        : {overview['shape']}\")\n",
    "        print(f\"Duplicated Rows      : {overview['duplicated_rows']}\")\n",
    "        print(f\"Total Missing Values : {overview['missing_values']}\")\n",
    "        print(f\"Data Types           : {overview['data_types']}\")\n",
    "        \n",
    "        print(\"\\nℹ️ STRUCTURE (.info()):\")\n",
    "        buffer = io.StringIO()\n",
    "        df.info(buf=buffer)      \n",
    "        print(buffer.getvalue())\n",
    "        \n",
    "        # Handle empty DataFrame\n",
    "        if len(df) == 0:\n",
    "            print(\"\\n⚠️ Empty DataFrame: No statistics or samples available\")\n",
    "            self.log_step(\"Data Overview Complete\", f\"Shape: {overview['shape']} (empty)\")\n",
    "            return overview\n",
    "        \n",
    "        # Display sample data and basic statistics\n",
    "        print(\"\\n📋 BASIC STATISTICS:\")\n",
    "        display(df.describe(include='all').round(2))\n",
    "        print(f\"\\n🎲 RANDOM SAMPLE ({sample_size} rows):\")\n",
    "        display(df.sample(min(sample_size, len(df))))\n",
    "        \n",
    "        # Display missing values by column and percentage\n",
    "        print(\"\\n❌ MISSING VALUES BY COLUMN:\")\n",
    "        missing_data = df.isnull().sum()\n",
    "        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "        if len(missing_data) > 0:\n",
    "            missing_pct = (missing_data / len(df)) * 100\n",
    "            missing_df = pd.DataFrame({\n",
    "                'Missing_Count'     : missing_data,\n",
    "                'Missing_Percentage': missing_pct.round(2)\n",
    "            })\n",
    "            print(missing_df)\n",
    "        else:\n",
    "            print(\"No missing values found! 🎉\")\n",
    "        \n",
    "        # record the overview step\n",
    "        self.log_step(\"Data Overview Complete\", f\"Shape: {overview['shape']}\")\n",
    "        return overview\n",
    "    \n",
    "    def plot_missing_data(self, df):\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"🎉 No missing data to visualize.\")\n",
    "            self.log_step(\"Plot Missing Data\", \"No missing values found\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            import missingno as msno\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            msno.matrix(df)\n",
    "            plt.title(\"Missing Data Pattern\")\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            msno.bar(df)\n",
    "            plt.title(\"Missing Data Count by Column\")\n",
    "            plt.show()\n",
    "            \n",
    "            self.log_step(\"Plotted Missing Data\", \"using missingno\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"⚠️ Install missingno for better visualization: pip install missingno\")\n",
    "            missing_data = df.isnull().sum()\n",
    "            missing_data = missing_data[missing_data > 0]\n",
    "            if len(missing_data) > 0:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                missing_data.plot(kind='bar')\n",
    "                plt.title(\"Missing Values by Column\")\n",
    "                plt.xlabel(\"Columns\")\n",
    "                plt.ylabel(\"Missing Count\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                self.log_step(\"Plotted Missing Data\", \"fallback with matplotlib\")\n",
    "    # =================== 2. HANDLING MISSING DATA ===================\n",
    "    def _auto_missing_strategy(self, df: pd.DataFrame) -> Dict[str, str]:\n",
    "        strategy = {}\n",
    "        \n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                    non_null_data = df[column].dropna()\n",
    "                    if len(non_null_data) > 8:\n",
    "                        try:\n",
    "                            _, p_value = stats.normaltest(non_null_data)\n",
    "                            strategy[column] = 'mean' if p_value > 0.05 else 'median'\n",
    "                        except Exception:\n",
    "                            strategy[column] = 'median'\n",
    "                    else:\n",
    "                        strategy[column] = 'median'\n",
    "                else:\n",
    "                    strategy[column] = 'most_frequent'\n",
    "        self.log_step(\"Auto Imputation Strategy\", f\"{len(strategy)} columns will be imputed\")\n",
    "        return strategy\n",
    "    \n",
    "    def handle_missing_data(self, df: pd.DataFrame, strategy: Dict[str, str] = None, drop_threshold: float = 0.7, advanced_imputation: bool = False) -> pd.DataFrame:\n",
    "        df_processed = df.copy()\n",
    "        print(f\"\\n{'=' * 20} 🔧 HANDLING MISSING DATA {'=' * 20}\")\n",
    "        \n",
    "        # Drop columns with excessive missing values (drop_threshold= 70% default)\n",
    "        missing_pct  = df_processed.isnull().sum() / len(df_processed)\n",
    "        cols_to_drop = missing_pct[missing_pct > drop_threshold].index.tolist()\n",
    "        if cols_to_drop:\n",
    "            df_processed = df_processed.drop(columns=cols_to_drop)\n",
    "            self.log_step(\"Dropped columns\", f\"{cols_to_drop} (>{drop_threshold*100}% missing)\")\n",
    "        \n",
    "        # Apply imputation strategies\n",
    "        if strategy is None:\n",
    "            strategy = self._auto_missing_strategy(df_processed)\n",
    "        \n",
    "        for column, method in strategy.items():\n",
    "            if column not in df_processed.columns:\n",
    "                continue\n",
    "            if method == 'drop':\n",
    "                df_processed = df_processed.dropna(subset=[column])\n",
    "                self.log_step(f\"Dropped rows for {column}\", \"NaN rows removed\")\n",
    "            elif method in ['mean', 'median', 'most_frequent']:\n",
    "                if df_processed[column].dtype in ['int64', 'float64']:\n",
    "                    imputer = SimpleImputer(strategy=method)\n",
    "                    df_processed[column]  = imputer.fit_transform(df_processed[[column]]).ravel()\n",
    "                    self.imputers[column] = imputer\n",
    "                    self.log_step(f\"Imputed {column}\", f\"Strategy: {method}\")\n",
    "            elif method in ['ffill', 'bfill']:\n",
    "                df_processed[column] = df_processed[column].fillna(method=method)\n",
    "                self.log_step(f\"Forward/Backward fill {column}\", f\"Method: {method}\")\n",
    "        \n",
    "        # Advanced imputation for numerical columns\n",
    "        if advanced_imputation:\n",
    "            numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "            missing_numeric = [col for col in numeric_cols if df_processed[col].isnull().any()]\n",
    "            \n",
    "            if missing_numeric:\n",
    "                knn_imputer = KNNImputer(n_neighbors=5)\n",
    "                df_processed[missing_numeric] = knn_imputer.fit_transform(df_processed[missing_numeric])\n",
    "                self.imputers['knn_numeric'] = knn_imputer\n",
    "                self.log_step(\"KNN Imputation\", f\"Applied to {missing_numeric}\")\n",
    "        \n",
    "        return df_processed\n",
    "    # =================== 3. HANDLING OUTLIERS ===================\n",
    "    def detect_outliers(self, df: pd.DataFrame, method: str = 'iqr', columns: List[str] = None) -> Dict[str, List[int]]:\n",
    "        outliers = {}\n",
    "        \n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        for column in columns:\n",
    "            if not np.issubdtype(df[column].dtype, np.number):\n",
    "                continue\n",
    "            series = df[column] \n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = series.quantile(0.25)\n",
    "                Q3 = series.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                outlier_mask = (series < lower_bound) | (series > upper_bound)\n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs(stats.zscore(series.fillna(series.mean()))) \n",
    "                outlier_mask = z_scores > 3\n",
    "            elif method == 'modified_zscore':\n",
    "                median = series.median()\n",
    "                mad = np.median(np.abs(series - median))\n",
    "                if mad == 0:\n",
    "                    outliers[column] = []\n",
    "                    continue\n",
    "                modified_z_scores = 0.6745 * (series - median) / mad\n",
    "                outlier_mask = np.abs(modified_z_scores) > 3.5\n",
    "            outliers[column] = series[outlier_mask].index.tolist()\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'Column': list(outliers.keys()),\n",
    "            'Num_Outliers': [len(idxs) for idxs in outliers.values()],\n",
    "            'Outlier_Indices': list(outliers.values())\n",
    "        })\n",
    "        display(summary_df) \n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    def handle_outliers(self, df: pd.DataFrame, method: str = 'winsorize', detection_method: str = 'iqr',\n",
    "                        columns: List[str] = None, percentiles: Tuple[float, float] = (0.01, 0.99)) -> pd.DataFrame:\n",
    "        df_processed = df.copy()\n",
    "        print(f\"\\n{'=' * 20} 🎯 HANDLING OUTLIERS {'=' * 20}\")\n",
    "        if columns is None:\n",
    "            columns = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        for column in columns:\n",
    "            if column not in df_processed.columns:\n",
    "                continue\n",
    "            if df_processed[column].dropna().empty:\n",
    "                continue  \n",
    "            \n",
    "            if method == 'remove':\n",
    "                outliers = self.detect_outliers(df_processed, detection_method, [column])\n",
    "                df_processed = df_processed.drop(index=outliers[column])\n",
    "                self.log_step(f\"Removed outliers from {column}\", f\"Removed {len(outliers[column])} rows\")\n",
    "            \n",
    "            elif method == 'winsorize':\n",
    "                lower_pct, upper_pct = percentiles\n",
    "                lower_bound = df_processed[column].quantile(lower_pct)\n",
    "                upper_bound = df_processed[column].quantile(upper_pct)\n",
    "                df_processed[column] = df_processed[column].clip(lower_bound, upper_bound)\n",
    "                self.log_step(f\"Winsorized {column:<8}\", f\"Bounds : [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "            \n",
    "            elif method == 'log_transform':\n",
    "                col_values = df_processed[column].dropna()\n",
    "                if (col_values > 0).all():\n",
    "                    df_processed[column] = np.log1p(df_processed[column])\n",
    "                    self.log_step(f\"Log transformed {column}\", \"Applied log1p transformation\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Cannot log-transform '{column}' — contains non-positive or NaN values\")\n",
    "            \n",
    "            elif method == 'cap':\n",
    "                Q1 = df_processed[column].quantile(0.25)\n",
    "                Q3 = df_processed[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                df_processed[column] = df_processed[column].clip(lower_bound, upper_bound)\n",
    "                self.log_step(f\"Capped {column}\", \"IQR bounds applied\")\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def plot_outliers(self, df: pd.DataFrame, columns: List[str] = None, save_path: str = None):\n",
    "        if columns is None:\n",
    "            columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        n_cols = min(3, len(columns))\n",
    "        n_rows = (len(columns) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "        fig.suptitle(\"Boxplots of Outlier Detection\", fontsize=16, y=1.02)\n",
    "        \n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, column in enumerate(columns):\n",
    "            sns.boxplot(data=df, y=column, ax=axes[i])\n",
    "            axes[i].set_title(f'Outliers in {column}')\n",
    "        \n",
    "        for i in range(len(columns), len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    # =================== 4. DATA TYPE CONVERSION ===================\n",
    "    def optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_optimized = df.copy()\n",
    "        print(f\"\\n {'=' * 20} 🔧 OPTIMIZING DATA TYPES {'=' * 20}\")\n",
    "        \n",
    "        original_memory = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        # Optimize integers\n",
    "        int_columns = df_optimized.select_dtypes(include=['int64']).columns\n",
    "        for col in int_columns:\n",
    "            col_min = df_optimized[col].min()\n",
    "            col_max = df_optimized[col].max()\n",
    "            \n",
    "            if col_min >= 0:\n",
    "                if col_max < 255:\n",
    "                    df_optimized[col] = df_optimized[col].astype('uint8')\n",
    "                elif col_max < 65535:\n",
    "                    df_optimized[col] = df_optimized[col].astype('uint16')\n",
    "                elif col_max <= 4294967295:\n",
    "                    df_optimized[col] = df_optimized[col].astype('uint32')\n",
    "            else:\n",
    "                if col_min > -128 and col_max < 127:\n",
    "                    df_optimized[col] = df_optimized[col].astype('int8')\n",
    "                elif col_min > -32768 and col_max < 32767:\n",
    "                    df_optimized[col] = df_optimized[col].astype('int16')\n",
    "                elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                    df_optimized[col] = df_optimized[col].astype('int32')\n",
    "        \n",
    "        # Optimize floats\n",
    "        float_columns = df_optimized.select_dtypes(include=['float64']).columns\n",
    "        for col in float_columns:\n",
    "            df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='float')\n",
    "        \n",
    "        # Convert to category for low cardinality objects\n",
    "        object_columns = df_optimized.select_dtypes(include=['object']).columns\n",
    "        for col in object_columns:\n",
    "            unique_ratio = df_optimized[col].nunique() / len(df_optimized)\n",
    "            if unique_ratio < 0.5:\n",
    "                df_optimized[col] = df_optimized[col].astype('category')\n",
    "        \n",
    "        optimized_memory = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "        memory_reduction = (1 - optimized_memory / original_memory) * 100\n",
    "        \n",
    "        self.log_step(\"Data types optimized\", f\"Memory reduction: {memory_reduction:.1f}% ({original_memory:.1f}MB → {optimized_memory:.1f}MB)\")\n",
    "        \n",
    "        return df_optimized\n",
    "    \n",
    "    def convert_datetime(self, df: pd.DataFrame, datetime_columns: List[str], format: str = None) -> pd.DataFrame:\n",
    "        df_processed = df.copy()\n",
    "        for col in datetime_columns:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = pd.to_datetime(df_processed[col], format=format, errors='coerce')\n",
    "                self.log_step(f\"Converted {col} to datetime\", f\"Format: {format or 'inferred'}\")\n",
    "                \n",
    "                num_nats = df_processed[col].isna().sum()\n",
    "                if num_nats > 0:\n",
    "                    self.log_step(f\"⚠️ {num_nats} NaT values in {col}\", \"Some rows couldn't be converted\")\n",
    "            else:\n",
    "                self.log_step(f\"Column {col} not found\", \"⚠️ Skipped conversion\")\n",
    "        \n",
    "        return df_processed\n",
    "    # =================== 5. ENCODING CATEGORICAL VARIABLES ===================\n",
    "    def _auto_encoding_strategy(self, df: pd.DataFrame, threshold: int) -> Dict[str, str]:\n",
    "        strategy = {}\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for column in categorical_columns:\n",
    "            cardinality = df[column].nunique()\n",
    "            strategy[column] = 'label' if cardinality == 2 or cardinality > threshold else 'onehot'\n",
    "        return strategy\n",
    "    \n",
    "    def encode_categorical(self, df: pd.DataFrame, encoding_strategy: Dict[str, str] = None,high_cardinality_threshold: int = 10) -> pd.DataFrame:\n",
    "        df_encoded = df.copy()\n",
    "        print(f\"\\n{'=' * 20} 🏷️ ENCODING CATEGORICAL VARIABLES {'=' * 20}\")\n",
    "        categorical_columns = df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        if encoding_strategy is None:\n",
    "            encoding_strategy = self._auto_encoding_strategy(df_encoded, high_cardinality_threshold)\n",
    "        \n",
    "        for column, method in encoding_strategy.items():\n",
    "            if column not in categorical_columns:\n",
    "                continue\n",
    "            if method == 'label':\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[column] = le.fit_transform(df_encoded[column].astype(str))\n",
    "                self.encoders[f'{column}_label'] = le\n",
    "                self.log_step(f\"Label encoded {column}\", f\"Classes: {len(le.classes_)}\")\n",
    "            elif method == 'onehot':\n",
    "                dummies    = pd.get_dummies(df_encoded[column], prefix=column, drop_first=True)\n",
    "                df_encoded = pd.concat([df_encoded.drop(column, axis=1), dummies], axis=1)\n",
    "                self.log_step(f\"One-hot encoded {column}\", f\"Created {len(dummies.columns)} features\")\n",
    "            elif method == 'target':\n",
    "                self.log_step(f\"Target encoding for {column}\", \"Skipped - requires target variable\")\n",
    "        \n",
    "        return df_encoded\n",
    "    # =================== 6. FEATURE SCALING ===================\n",
    "    def scale_features(self, df: pd.DataFrame, method: str = 'standard', columns: List[str] = None) -> pd.DataFrame:\n",
    "        df_scaled = df.copy()\n",
    "        print(f\"\\n{'=' * 20} SCALING FEATURES {'=' * 20}\")\n",
    "        if columns is None:\n",
    "            columns = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if method   == 'standard':\n",
    "            scaler  = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            scaler  = MinMaxScaler()\n",
    "        elif method == 'robust':\n",
    "            scaler  = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'standard', 'minmax', or 'robust'\")\n",
    "        df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n",
    "        self.scalers[method] = scaler\n",
    "        self.log_step(f\"{method.title()} scaling applied\", f\"Scaled {len(columns)} features\")\n",
    "        \n",
    "        return df_scaled\n",
    "    # =================== 7. FEATURE ENGINEERING ===================\n",
    "    def create_polynomial_features(self, df: pd.DataFrame, columns: List[str], degree: int = 2,interaction_only: bool = False) -> pd.DataFrame:\n",
    "        df_poly = df.copy()\n",
    "        poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=False)\n",
    "        poly_features      = poly.fit_transform(df[columns])\n",
    "        poly_feature_names = poly.get_feature_names_out(columns)\n",
    "        for i, name in enumerate(poly_feature_names):\n",
    "            if name not in columns:\n",
    "                df_poly[name] = poly_features[:, i]\n",
    "        self.log_step(\"Polynomial features created\", f\"Degree: {degree}, New features: {len(poly_feature_names) - len(columns)}\")\n",
    "        return df_poly\n",
    "    \n",
    "    def create_datetime_features(self, df: pd.DataFrame, datetime_columns: List[str]) -> pd.DataFrame:\n",
    "        df_dt = df.copy()\n",
    "        for col in datetime_columns:\n",
    "            if col in df_dt.columns and pd.api.types.is_datetime64_any_dtype(df_dt[col]):\n",
    "                df_dt[f'{col}_year']       = df_dt[col].dt.year\n",
    "                df_dt[f'{col}_month']      = df_dt[col].dt.month\n",
    "                df_dt[f'{col}_day']        = df_dt[col].dt.day\n",
    "                df_dt[f'{col}_dayofweek']  = df_dt[col].dt.dayofweek\n",
    "                df_dt[f'{col}_hour']       = df_dt[col].dt.hour\n",
    "                df_dt[f'{col}_is_weekend'] = (df_dt[col].dt.dayofweek >= 5).astype(int)\n",
    "                df_dt[f'{col}_quarter']    = df_dt[col].dt.quarter\n",
    "                df_dt[f'{col}_season']     = df_dt[col].dt.month % 12 // 3 + 1\n",
    "                self.log_step(f\"DateTime features created for {col}\", \"8 new features\")\n",
    "        return df_dt\n",
    "    \n",
    "    def create_binned_features(self, df: pd.DataFrame, column: str, bins: Union[int, List], labels: List[str] = None) -> pd.DataFrame:\n",
    "        df_binned  = df.copy()\n",
    "        binned_col = f'{column}_binned'\n",
    "        df_binned[binned_col] = pd.cut(df_binned[column], bins=bins, labels=labels)\n",
    "        self.log_step(f\"Created binned feature {binned_col}\", f\"Bins: {bins}\")\n",
    "        return df_binned\n",
    "    # =================== 8. FEATURE SELECTION ===================\n",
    "    def select_features(self, df: pd.DataFrame, target: str, method: str = 'correlation', k: int = 10) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        print(f\"\\n{'=' * 20} 🎯 FEATURE SELECTION {'=' * 20}\")\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "        \n",
    "        if method == 'correlation':\n",
    "            corr_matrix = X.corr().abs()\n",
    "            upper_tri   = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            to_drop     = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n",
    "            selected_features = [col for col in X.columns if col not in to_drop][:k]\n",
    "            \n",
    "        elif method == 'chi2':\n",
    "            selector = SelectKBest(score_func=chi2, k=k)\n",
    "            selector.fit(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            \n",
    "        elif method == 'f_classif':     # ANOVA F-Value\n",
    "            selector = SelectKBest(score_func=f_classif, k=k)\n",
    "            selector.fit(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            \n",
    "        elif method == 'rfe':\n",
    "            estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=k)\n",
    "            selector.fit(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            \n",
    "        elif method == 'lasso':\n",
    "            lasso = Lasso(alpha=0.01, random_state=42)\n",
    "            selector = SelectFromModel(lasso, max_features=k)\n",
    "            selector.fit(X, y)\n",
    "            selected_features = X.columns[selector.get_support()].tolist()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Method must be one of: 'correlation', 'chi2', 'f_classif', 'rfe', 'lasso'\")\n",
    "        selected_df = df[selected_features + [target]]\n",
    "        self.log_step(f\"Feature selection ({method})\", f\"Selected {len(selected_features)} features\")\n",
    "        return selected_df, selected_features\n",
    "    # =================== 9. DATA SPLITTING ===================\n",
    "    def split_data(self, df: pd.DataFrame, target: str, test_size: float = 0.2, val_size: Optional[float] = None,\n",
    "                stratify: bool = True, time_based: bool = False, date_column: Optional[str] = None,random_state: int = 42) -> Union[Tuple, Dict]:\n",
    "        print(f\"\\n{'=' * 20} DATA SPLITTING {'=' * 20}\")\n",
    "        X = df.drop(columns=[target])\n",
    "        y = df[target]\n",
    "        \n",
    "        if time_based:\n",
    "            if date_column is None or date_column not in df.columns:\n",
    "                raise ValueError(\"date_column must be specified for time-based splitting\")\n",
    "            df_sorted = df.sort_values(date_column)\n",
    "            X_sorted  = df_sorted.drop(columns=[target])\n",
    "            y_sorted  = df_sorted[target]\n",
    "            test_idx  = int(len(df_sorted) * (1 - test_size))\n",
    "            \n",
    "            if val_size:\n",
    "                val_idx = int(len(df_sorted) * (1 - test_size - val_size))\n",
    "                X_train = X_sorted.iloc[:val_idx]\n",
    "                y_train = y_sorted.iloc[:val_idx]\n",
    "                X_val   = X_sorted.iloc[val_idx:test_idx]\n",
    "                y_val   = y_sorted.iloc[val_idx:test_idx]\n",
    "                X_test  = X_sorted.iloc[test_idx:]\n",
    "                y_test  = y_sorted.iloc[test_idx:]\n",
    "                self.log_step(\"Time-based split with validation\", f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "                return {\n",
    "                    'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "                    'y_train': y_train, 'y_val': y_val, 'y_test': y_test\n",
    "                }\n",
    "            \n",
    "            else:\n",
    "                X_train = X_sorted.iloc[:test_idx]\n",
    "                y_train = y_sorted.iloc[:test_idx]\n",
    "                X_test  = X_sorted.iloc[test_idx:]\n",
    "                y_test  = y_sorted.iloc[test_idx:]\n",
    "                self.log_step(\"Time-based split\", f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "                return X_train, X_test, y_train, y_test\n",
    "        \n",
    "        stratify_param = y if stratify and len(y.unique()) > 1 else None\n",
    "        if val_size:\n",
    "            X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, stratify=stratify_param, random_state=random_state)\n",
    "            val_size_adjusted   = val_size / (1 - test_size)\n",
    "            stratify_temp       = y_temp if stratify and len(y_temp.unique()) > 1 else None\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size_adjusted, stratify=stratify_temp, random_state=random_state)\n",
    "            self.log_step(\"Three-way split\", f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "            return {\n",
    "                'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "                'y_train': y_train, 'y_val': y_val, 'y_test': y_test\n",
    "            }\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=stratify_param, random_state=random_state)\n",
    "        self.log_step(\"Train-test split\", f\"Train: {len(X_train)}, Test: {len(X_test)}, Stratified: {stratify}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def create_cross_validation_folds(self, X: pd.DataFrame, y: pd.Series, cv_type: str = 'kfold',n_splits: int = 5, shuffle: bool = True, random_state: int = 42) -> object:\n",
    "        if cv_type   == 'stratified':\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        elif cv_type == 'timeseries':\n",
    "            cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        else:\n",
    "            cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self.log_step(f\"{cv_type.title()} CV created\", f\"{n_splits} folds\")\n",
    "        return cv\n",
    "    # =================== 10. CLASS BALANCING ===================\n",
    "    def balance_classes(self, X: pd.DataFrame, y: pd.Series, method: str = 'smote',sampling_strategy: Union[str, dict] = 'auto', random_state: int = 42,**kwargs) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        print(f\"\\n{'=' * 20} ⚖️ CLASS BALANCING {'=' * 20}\")\n",
    "        original_dist = Counter(y)\n",
    "        print(f\"Original distribution: {dict(original_dist)}\")\n",
    "        \n",
    "        samplers = {\n",
    "            'smote'             : SMOTE(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'adasyn'            : ADASYN(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'borderline_smote'  : BorderlineSMOTE(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'random_over'       : RandomOverSampler(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'random_under'      : RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'tomek'             : TomekLinks(sampling_strategy=sampling_strategy, **kwargs),\n",
    "            'enn'               : EditedNearestNeighbours(sampling_strategy=sampling_strategy, **kwargs),\n",
    "            'smote_tomek'       : SMOTETomek(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs),\n",
    "            'smote_enn'         : SMOTEENN(sampling_strategy=sampling_strategy, random_state=random_state, **kwargs)}\n",
    "        \n",
    "        if method not in samplers:\n",
    "            raise ValueError(f\"Unknown balancing method: {method}\")\n",
    "        X_balanced, y_balanced = samplers[method].fit_resample(X, y)\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_balanced = pd.DataFrame(X_balanced, columns=X.columns)\n",
    "        \n",
    "        if isinstance(y, pd.Series):\n",
    "            y_balanced = pd.Series(y_balanced, name=y.name)\n",
    "        \n",
    "        new_dist = Counter(y_balanced)\n",
    "        print(f\"Balanced distribution: {dict(new_dist)}\")\n",
    "        \n",
    "        self.balancing_info = {\n",
    "            'method': method,\n",
    "            'original_distribution': dict(original_dist),\n",
    "            'balanced_distribution': dict(new_dist),\n",
    "            'original_size': len(y),\n",
    "            'balanced_size': len(y_balanced)}\n",
    "        self.log_step(f\"Class balancing ({method})\", f\"Size: {len(y)} → {len(y_balanced)}\")\n",
    "        return X_balanced, y_balanced\n",
    "    \n",
    "    def create_balanced_pipeline(self, balancing_method: str = 'smote', classifier=None,**balance_params) -> ImbPipeline:\n",
    "        if classifier is None:\n",
    "            classifier = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "        samplers = {\n",
    "            'smote'         : SMOTE(random_state=42, **balance_params),\n",
    "            'adasyn'        : ADASYN(random_state=42, **balance_params),\n",
    "            'random_over'   : RandomOverSampler(random_state=42, **balance_params)\n",
    "        }\n",
    "        sampler  = samplers.get(balancing_method, SMOTE(random_state=42, **balance_params))\n",
    "        pipeline = ImbPipeline([('balancer', sampler), ('classifier', classifier)])\n",
    "        self.log_step(\"Balanced pipeline created\", f\"Method: {balancing_method}, Classifier: {type(classifier).__name__}\")\n",
    "        return pipeline\n",
    "    \n",
    "    def evaluate_balanced_model(self, pipeline: ImbPipeline, X: pd.DataFrame, y: pd.Series,\n",
    "                                cv_folds: int = 5, scoring: List[str] = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']) -> Dict:\n",
    "        results = {}\n",
    "        for metric in scoring:\n",
    "            scores = cross_val_score(pipeline, X, y, cv=cv_folds, scoring=metric)\n",
    "            results[metric] = {\n",
    "                'mean'  : scores.mean(),\n",
    "                'std'   : scores.std(),\n",
    "                'scores': scores\n",
    "            }\n",
    "            print(f\"{metric.upper()}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "        self.log_step(\"Model evaluation completed\", f\"Metrics: {', '.join(scoring)}\")\n",
    "        return results\n",
    "    \n",
    "    def _get_balancing_recommendation(self, imbalance_ratio: float, total_samples: int) -> str:\n",
    "        if imbalance_ratio <= 1.5:\n",
    "            return \"No balancing needed\"\n",
    "        elif imbalance_ratio <= 4:\n",
    "            return \"Consider class weights or light oversampling\"\n",
    "        elif imbalance_ratio <= 10:\n",
    "            return \"Use SMOTE or ADASYN\"\n",
    "        elif total_samples < 1000:\n",
    "            return \"Use SMOTE with careful validation\"\n",
    "        else:\n",
    "            return \"Use advanced techniques like SMOTE + undersampling\"\n",
    "    \n",
    "    def detect_class_imbalance(self, y: pd.Series, imbalance_threshold: float = 0.1) -> Dict:\n",
    "        print(f\"\\n {'=' * 20} 📊 CLASS IMBALANCE ANALYSIS {'=' * 20}\")\n",
    "        \n",
    "        class_counts  = Counter(y)\n",
    "        total_samples = len(y)\n",
    "        class_proportions = {cls: count / total_samples for cls, count in class_counts.items()}\n",
    "        min_proportion    = min(class_proportions.values())\n",
    "        max_proportion    = max(class_proportions.values())\n",
    "        is_imbalanced     = min_proportion < imbalance_threshold\n",
    "        imbalance_ratio   = max_proportion / min_proportion if min_proportion > 0 else float('inf')\n",
    "        analysis = {\n",
    "            'is_imbalanced'         : is_imbalanced,\n",
    "            'class_counts'          : dict(class_counts),\n",
    "            'class_proportions'     : class_proportions,\n",
    "            'min_class_proportion'  : min_proportion,\n",
    "            'max_class_proportion'  : max_proportion,\n",
    "            'imbalance_ratio'       : imbalance_ratio,\n",
    "            'recommendation'        : self._get_balancing_recommendation(imbalance_ratio, total_samples)}\n",
    "        print(f\"Classes         : {len(class_counts)}\")\n",
    "        print(f\"Total samples   : {total_samples}\")\n",
    "        print(f\"Imbalanced      : {'Yes' if is_imbalanced else 'No'}\")\n",
    "        print(f\"Imbalance ratio : {imbalance_ratio:.2f}:1\")\n",
    "        print(f\"Recommendation  : {analysis['recommendation']}\")\n",
    "        return analysis\n",
    "    # =================== UTILITY METHODS ===================\n",
    "    def get_preprocessing_summary(self) -> pd.DataFrame:\n",
    "        steps_data = []\n",
    "        \n",
    "        if not self.steps_log:\n",
    "            return pd.DataFrame(columns=['Step', 'Details'])\n",
    "        \n",
    "        for i, step in enumerate(self.steps_log, 1):\n",
    "            parts     = step.replace('✅ ', '').split(': ', 1)\n",
    "            step_name = parts[0] if len(parts) > 0 else f\"Step {i}\"\n",
    "            details   = parts[1] if len(parts) > 1 else \"No details\"\n",
    "            steps_data.append({\n",
    "                'Step_Number': i,\n",
    "                'Step_Name'  : step_name,\n",
    "                'Details'    : details\n",
    "            })\n",
    "        return pd.DataFrame(steps_data)\n",
    "    \n",
    "    def export_preprocessing_config(self, filepath: str):\n",
    "        config = {\n",
    "            'preprocessing_steps': self.steps_log,\n",
    "            'encoders'           : {k: str(type(v)) for k, v in self.encoders.items()},\n",
    "            'scalers'            : {k: str(type(v)) for k, v in self.scalers.items()},\n",
    "            'feature_names'      : self.feature_names,\n",
    "            'balancing_info'     : self.balancing_info\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config, f, indent=2, default=str)\n",
    "        self.log_step(\"Configuration exported\", f\"Saved to {filepath}\")\n",
    "    \n",
    "    def reset_preprocessor(self):\n",
    "        self.steps_log      = []\n",
    "        self.encoders       = {}\n",
    "        self.scalers        = {}\n",
    "        self.feature_names  = []\n",
    "        self.balancing_info = {}\n",
    "        print(\"🔄 Preprocessor reset successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳ STARTING COMPLETE PREPROCESSING PIPELINE ⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳⏳\n",
      "\n",
      " ==================== 📊 CLASS IMBALANCE ANALYSIS ====================\n",
      "Classes         : 2\n",
      "Total samples   : 1000\n",
      "Imbalanced      : No\n",
      "Imbalance ratio : 8.62:1\n",
      "Recommendation  : Use SMOTE or ADASYN\n",
      "\n",
      "==================== DATA SPLITTING ====================\n",
      "✅ Three-way split: Train: 700, Val: 100, Test: 200\n",
      "\n",
      "==================== ⚖️ CLASS BALANCING ====================\n",
      "Original distribution: {0: 627, 1: 73}\n",
      "Balanced distribution: {0: 627, 1: 627}\n",
      "✅ Class balancing (smote): Size: 700 → 1254\n",
      "✅ Balanced pipeline created: Method: smote, Classifier: RandomForestClassifier\n",
      "ACCURACY: 0.9814 (±0.0147)\n",
      "F1_MACRO: 0.9495 (±0.0393)\n",
      "PRECISION_MACRO: 0.9579 (±0.0364)\n",
      "RECALL_MACRO: 0.9417 (±0.0429)\n",
      "✅ Model evaluation completed: Metrics: accuracy, f1_macro, precision_macro, recall_macro\n",
      "\n",
      "📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋 PREPROCESSING SUMMARY 📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋📋\n",
      " Step_Number                  Step_Name                                                    Details\n",
      "           1            Three-way split                            Train: 700, Val: 100, Test: 200\n",
      "           2    Class balancing (smote)                                           Size: 700 → 1254\n",
      "           3  Balanced pipeline created          Method: smote, Classifier: RandomForestClassifier\n",
      "           4 Model evaluation completed Metrics: accuracy, f1_macro, precision_macro, recall_macro\n",
      "✅ Configuration exported: Saved to preprocessing_config.json\n",
      "\n",
      "🎉 Preprocessing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def example_usage():\n",
    "    from sklearn.datasets import make_classification\n",
    "    preprocessor = MLPreprocessor()\n",
    "    \n",
    "    # Create imbalanced dataset\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2, n_clusters_per_class=1,weights=[0.9, 0.1], random_state=42)\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    df['target'] = y\n",
    "    print(f\"{'⏳' * 20} STARTING COMPLETE PREPROCESSING PIPELINE {'⏳' * 20}\")\n",
    "    \n",
    "    # Analyze class imbalance\n",
    "    preprocessor.detect_class_imbalance(df['target'])\n",
    "    \n",
    "    # Split data with validation set\n",
    "    splits = preprocessor.split_data(df, target='target', test_size=0.2, val_size=0.1, stratify=True)\n",
    "    X_train, X_val, X_test = splits['X_train'], splits['X_val'], splits['X_test']\n",
    "    y_train, y_val, y_test = splits['y_train'], splits['y_val'], splits['y_test']\n",
    "    \n",
    "    # Balance training data\n",
    "    X_train_balanced, y_train_balanced = preprocessor.balance_classes(X_train, y_train, method='smote')\n",
    "    \n",
    "    # Create and evaluate balanced pipeline\n",
    "    pipeline = preprocessor.create_balanced_pipeline(balancing_method='smote',classifier=RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    evaluation_results = preprocessor.evaluate_balanced_model(pipeline, X_train, y_train, cv_folds=5)\n",
    "    \n",
    "    # Display preprocessing summary\n",
    "    summary = preprocessor.get_preprocessing_summary()\n",
    "    print(f\"\\n{'📋' * 20} PREPROCESSING SUMMARY {'📋' * 20}\")\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # Export configuration\n",
    "    preprocessor.export_preprocessing_config('preprocessing_config.json')\n",
    "    return preprocessor, evaluation_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessor, results = example_usage()\n",
    "    print(\"\\n🎉 Preprocessing pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
